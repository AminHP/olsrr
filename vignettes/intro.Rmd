---
title: "Introduction to olsrr"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to olsrr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style type="text/css">
code.r{ /* Code block */
  font-size: 12px;
}
#best-subset-regression pre { /* Code block */
  font-size: 10px
}
</style>

The olsrr package makes some part of model building process easy by:

- generating comprehensive regression output
- demonstrating multiple model selection methods
- providing multiple residual diagnostics
- validating various model assumptions

```{r, echo=FALSE, message=FALSE}
library(olsrr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(purrr)
library(tibble)
library(nortest)
library(goftest)
```

## Regression

```{r regress}
regress(mpg ~ disp + hp + wt + qsec, data = mtcars)
```

## Residual Diagnostics

### Residual QQ Plot

Graph for detecting violation of normality assumption.

```{r qqresid, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
qqresid(model)
```

### Residual Histogram

Histogram of residuals for detecting violation of normality assumption.

```{r residhist, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
hist_resid(model)
```

### Residual Box Plot

A box plot of residuals is used to detect violation of the homoscedasticity assumption.

```{r residbox, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
box_resid(model)
```


### Residual Normality Test

Test for detecting violation of normality assumption.

```{r normtest}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
norm_test(model)
```

Correlation between observed residuals and expected residuals under normality.

```{r corrtest}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
corr_test(model)
```

### Residual vs Regressor Plots

Graph to determine whether we should add a new predictor to the model already containing other predictors.
The residuals from the model is regressed on the new predictor and if the plot shows non random pattern, 
you should consider adding the new predictor to the model.

```{r rvsrplot, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt, data = mtcars)
rvsr_plot(model, mtcars$drat)
```

#### Residual vs Fitted Values Plot

Plot to detect non-linearity, unequal error variances, and outliers.

```{r rvsfplot, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
rvsp_plot(model)
```

## Measure of Influence

### Cook's D Bar Plot

Bar Plot of Cook's distance to detect observations that strongly influence fitted values of the model.

```{r ckdbp, fig.width=7, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
cooksd_bplot(model)
```

### Cook's D Chart

Chart of Cook's distance to detect observations that strongly influence fitted values of the model.

```{r ckchart, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
cooksd_chart(model)
```

### DFBETAs Panel

DFBETAs measure the difference in each parameter estimate with and without the influential observation.
`dfbetas_panel` creates plots to detect influential observations using DFBETAs.

```{r dfbpanel, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt, data = mtcars)
dfbetas_panel(model)
```

### DFFITS Plot

Proposed by Welsch and Kuh (1977). It is the scaled difference between the
$i^{th}$ fitted value obtained from the full data and the $i^{th}$ fitted
value obtained by deleting the $i^{th}$ observation.

```{r dfitsplot, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
dffits_plot(model)
```

### Studentized Residual Plot

Plot for detecting outliers.

```{r srplot, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
srplot(model)
```

### Studentized Residual Chart

Chart for detecting outliers.

```{r srchart, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
studres_chart(model)
```

### Studentized Residuals vs Leverage Plot

Graph for detecting influential observations.

```{r studlev, fig.width=7, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
studvslev_plot(model)
```

### Deleted Studentized Residual vs Fitted Values Plot

```{r dsrvsp, fig.width=7, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
dsrvsp_plot(model)
```

### Hadi Plot

Hadi's measure of influence based on the fact that influential observations can be present in either
the response variable or in the predictors or both. The plot is used to detect influential observations
based on Hadi's measure.

```{r hadiplot, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
hadi_plot(model)
```

### Potential Residual Plot

Plot to aid in classifying unusual observations as high-leverage points, outliers, or a combination of both.

```{r potres, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
poten_resid_plot(model)
```

## Model Fit Assessment

### Residual Fit Spread Plot

Plot to detect non-linearity, influential observations and outliers.

```{r rfsplot, fig.width=10, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
rfs_plot(model)
```

### Part & Partial Correlations

#### Correlations

Relative importance of independent variables in determining **Y**. How much each
variable uniquely contributes to $R^{2}$ over and above that which can be
accounted for by the other predictors.

##### Zero Order

Pearson correlation coefficient between the dependent variable and the
independent variables.

##### Part

Unique contribution of independent variables. How much $R^{2}$ will decrease if
that variable is removed from the model?

##### Partial

How much of the variance in **Y**, which is not estimated by the other
independent variables in the model, is estimated by the specific variable?

```{r partcor}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
correlations(model)
```

### Observed vs Predicted Plot

Plot of observed vs fitted values to assess the fit of the model.

```{r obspred, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
ovsp_plot(model)
```

### Lack of Fit F Test

Assess how much of the error in prediction is due to lack of model fit.

```{r lackfit}
model <- lm(mpg ~ disp, data = mtcars)
pure_error_anova(model)
```

### Diagnostics Panel

Panel of plots for regression diagnostics

```{r diagpanel, fig.width=10, fig.height=25, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
diag_panel(model)
```


## Variable Contributions

### Added Variable Plot

Added variable plot provides information about the marginal importance of a 
predictor variable $X_{k}$, given the other predictor variables already in 
the model. It shows the marginal importance of the variable in reducing the
residual variability. 

```{r avplot, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
addvar_plot(model)
```

### Residual Plus Component Plot

The residual plus component plot indicates whether any non-linearity is present in the relationship between 
response and predictor variables and can suggest possible transformations for linearizing the data.

```{r cplusr, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
cplusr_plot(model)
```

## Heteroskedasticity

### Bartlett Test

Bartlett's test is used to test if variances across samples is equal. It is sensitive to 
departures from normality. The Levene test is an alternative test that is less sensitive 
to departures from normality.

#### Use grouping variable
```{r bartlett1}
model <- lm(mpg ~ disp + hp, data = mtcars)
resid <- residuals(model)
cyl <- as.factor(mtcars$cyl)
bartlett_test(resid, group_var = cyl)
```

#### Using variables
```{r bartlett2}
bartlett_test(hsb$read, hsb$write)
```

#### Using formula
```{r bartlett3}
mt <- mtcars
mt$cyl <- as.factor(mt$cyl)
bartlett_test(mpg ~ cyl, data = mt)
```

#### Using linear model
```{r bartlett4}
model <- lm(mpg ~ cyl, data = mt)
bartlett_test(model)
```

### Breusch Pagan Test

Breusch Pagan test is used to test for herteroskedasticity (non-constant error variance). It tests whether the variance of the errors from a regression is dependent on the values of the independent variables. It is a $\chi^{2}$ test.

#### Use fitted values of the model

```{r bp1}
model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
bp_test(model)
```

#### Use independent variables of the model

```{r bp2}
model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
bp_test(model, rhs = TRUE)
```

#### Use independent variables of the model and perform multiple tests

```{r bp3}
model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
bp_test(model, rhs = TRUE, multiple = TRUE)
```

#### Bonferroni p value Adjustment

```{r bp4}
model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
bp_test(model, rhs = TRUE, multiple = TRUE, p.adj = 'bonferroni')
```

#### Sidak p value Adjustment

```{r bp5}
model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
bp_test(model, rhs = TRUE, multiple = TRUE, p.adj = 'sidak')
```

#### Holm's p value Adjustment

```{r bp6}
model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)
bp_test(model, rhs = TRUE, multiple = TRUE, p.adj = 'holm')
```

### Score Test

Test for heteroskedasticity under the assumption that the errors are independent and identically distributed (i.i.d.).

#### Use fitted values of the model

```{r score1}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
score_test(model)
```

#### Use independent variables of the model

```{r score2}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
score_test(model, rhs = TRUE)
```

#### Specify variables

```{r score3}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
score_test(model, vars = c('disp', 'hp'))
```

### F Test

F Test for heteroskedasticity under the assumption that the errors are independent and identically distributed (i.i.d.).

#### Use fitted values of the model

```{r ftest1}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
f_test(model)
```

#### Use independent variables of the model

```{r ftest2}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
f_test(model, rhs = TRUE)
```

#### Specify variables

```{r ftest3}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
f_test(model, vars = c('disp', 'hp'))
```

## Collinearity Diagnostics

Collinearity implies two variables are near perfect linear combinations of one
another. Multicollinearity involves more than two variables. In the presence of multicollinearity, 
regression estimates are unstable and have high standard errors.

### VIF

Variance inflation factors measure the inflation in the variances of the
parameter estimates due to collinearities that exist among the predictors.
It is a measure of how much the variance of the estimated regression coefficient
$\beta_{k}$ is "inflated" by the existence of correlation among the predictor
variables in the model. A VIF of 1 means that there is no correlation among the
kth predictor and the remaining predictor variables, and hence the variance of
$\beta_{k}$ is not inflated at all. The general rule of thumb is that VIFs
exceeding 4 warrant further investigation, while VIFs exceeding 10 are signs of
serious multicollinearity requiring correction.

Steps to calculate VIF:

- Regress the $k^{th}$ predictor on rest of the predictors in the model.
- Compute the ${R}^{2}_{k}$

$$VIF = \frac{1}{1 - {R}^{2}_{k}} = \frac{1}{Tolerance}$$

```{r viftol}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
vif_tol(model)
```

### Tolerance

Percent of variance in the predictor that cannot be accounted for by other
predictors.

Steps to calculate tolerance:

- Regress the $k^{th}$ predictor on rest of the predictors in the model.
- Compute the ${R}^{2}_{k}$

$$Tolerance = 1 - {R}^{2}_{k}$$


### Condition Index

Most multivariate statistical approaches involve decomposing a correlation matrix into linear combinations of variables. The linear combinations are chosen so that the first combination has the largest possible variance (subject to some restrictions we won't discuss), the second combination has the next largest variance, subject to being uncorrelated with the first, the third has the largest possible variance, subject to being uncorrelated with the first and second, and so forth. The variance of each of these linear combinations is called an eigenvalue. Collinearity is spotted by finding 2 or more variables that have large proportions of variance (.50 or more) that correspond to large condition indices. A rule of thumb is to label as large those condition indices in the range of 30 or larger.


```{r cindex}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
eigen_cindex(model)
```

### Collinearity Diagnostics

```{r colldiag}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
coll_diag(model)
```

## Variable Selection Procedures

### All Subset Regression

All subset regression tests all possible subsets of the set of potential independent variables. If there are K potential independent variables (besides the constant), then there are $2^{k}$ distinct subsets of them to be tested. For example, if you have 10 candidate independent variables, the number of subsets to be tested is $2^{10}$, which is 1024, and if you have 20 candidate variables, the number is $2^{20}$, which is more than one million.

```{r allsub}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
all_subset(model)
```

The `plot` method shows the panel of fit criteria for all possible regression methods. 

```{r allsubplot, fig.width=10, fig.height=15, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
k <- all_subset(model)
plot(k)
```

### Best Subset Regression

Select the subset of predictors that do the best at meeting some well-defined objective criterion, 
such as having the largest R2 value or the smallest MSE, Mallow's Cp or AIC.

```{r bestsub, size='tiny'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
best_subset(model)
```

The `plot` method shows the panel of fit criteria for best subset regression methods. 

```{r bestsubplot, fig.width=10, fig.height=15, fig.align='center'}
model <- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
k <- best_subset(model)
plot(k)
```

### Stepwise Forward Regression

Build regression model from a set of candidate predictor variables by entering predictors based on 
p values, in a stepwise manner until there is no variable left to enter any more.

#### Variable Selection

```{r stepf1}
# stepwise forward regression
model <- lm(y ~ ., data = surgical)
step_forward(model)
```

#### Plot

```{r stepf2, fig.width=10, fig.height=15, fig.align='center'}
model <- lm(y ~ ., data = surgical)
k <- step_forward(model)
plot(k)
```

### Stepwise Backward Regression

Build regression model from a set of candidate predictor variables by removing predictors based on 
p values, in a stepwise manner until there is no variable left to remove any more.

#### Variable Selection

```{r stepb, fig.width=10, fig.height=15, fig.align='center'}
# stepwise backward regression
model <- lm(y ~ ., data = surgical)
step_backward(model)
```

#### Plot

```{r stepb2, fig.width=10, fig.height=15, fig.align='center'}
model <- lm(y ~ ., data = surgical)
k <- step_backward(model)
plot(k)
```

### Stepwise Regression

Build regression model from a set of candidate predictor variables by entering and removing predictors based on 
p values, in a stepwise manner until there is no variable left to enter or remove any more.

#### Variable Selection

```{r stepwise1}
# stepwise regression
model <- lm(y ~ ., data = surgical)
stepwise(model)
```

#### Plot

```{r stepwise2, fig.width=10, fig.height=15, fig.align='center'}
model <- lm(y ~ ., data = surgical)
k <- stepwise(model)
plot(k)
```

### Stepwise AIC Forward Regression

Build regression model from a set of candidate predictor variables by entering predictors based on 
Akaike Information Criteria, in a stepwise manner until there is no variable left to enter any more.

#### Variable Selection

```{r stepaicf1}
# stepwise aic forward regression
model <- lm(y ~ ., data = surgical)
stepaic_forward(model)
```

#### Plot

```{r stepaicf2, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(y ~ ., data = surgical)
k <- stepaic_forward(model)
plot(k)
```

### Stepwise AIC Backward Regression

Build regression model from a set of candidate predictor variables by removing predictors based on 
Akaike Information Criteria, in a stepwise manner until there is no variable left to remove any more.

#### Variable Selection

```{r stepaicb1}
# stepwise aic backward regression
model <- lm(y ~ ., data = surgical)
k <- stepaic_backward(model)
k

#### Plot

```{r stepaicb2, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(y ~ ., data = surgical)
k <- stepaic_backward(model)
plot(k)
```

### Stepwise AIC Regression

Build regression model from a set of candidate predictor variables by entering and removing predictors based on 
Akaike Information Criteria, in a stepwise manner until there is no variable left to enter or remove any more.

#### Variable Selection

```{r stepwiseaic1}
# stepwise aic backward regression
model <- lm(y ~ ., data = surgical)
stepaic_both(model)
```

#### Plot

```{r stepwiseaic2, fig.width=5, fig.height=5, fig.align='center'}
model <- lm(y ~ ., data = surgical)
k <- stepaic_both(model)
plot(k)
```
